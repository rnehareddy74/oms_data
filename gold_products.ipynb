{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95652e04-1f5a-459b-93ce-9d9394a11adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import*\n",
    "from pyspark.sql.types import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e46d60-e0d6-4611-8299-07d12d48618b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#expectations\n",
    "my_expectations={\n",
    "    \"rule1\":\"product_id IS NOT NULL\",\n",
    "    \"rule2\":\"product_name IS NOT NULL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57712f73-fc99-417f-a4cf-fb64d9e1dff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>dim_products_stage</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>product_id</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>product_name</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>category</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>brand</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>price</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>discount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=1306486970388480#joblist/pipelines/create?initialSource=%2Fazureomsproject%2Fgold_products&redirectNotebookId=3682909926870584\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dlt.table()\n",
    "@dlt.expect_all_or_drop(my_expectations)\n",
    "def dim_products_stage():\n",
    "    df=spark.readStream.table(\"oms_catalog.silver.products_silver\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e6e01f-40dc-42d7-b4e0-0d11098ffe4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>dim_products_view</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>product_id</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>product_name</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>category</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>brand</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>price</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>discount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=1306486970388480#joblist/pipelines/create?initialSource=%2Fazureomsproject%2Fgold_products&redirectNotebookId=3682909926870584\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dlt.view()\n",
    "def dim_products_view():\n",
    "    df=spark.readStream.table(\"LIVE.dim_products_stage\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e812122e-31fc-4718-86c0-54fef95a5de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>dim_products</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       ".\n",
       "</div>\n",
       "\n",
       "  \n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=1306486970388480#joblist/pipelines/create?initialSource=%2Fazureomsproject%2Fgold_products&redirectNotebookId=3682909926870584\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dlt.create_streaming_table(\"dim_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70f2ea5-f6e4-40e1-81ff-834f08b0452f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 739, in apply\n    return self.df_func()._jdf\n           ^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 1011, in <lambda>\n    _ScalaDataFrameFunc(lambda: read_stream(source)),\n                                ^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 627, in read_stream\n    pipeline.instance.get_scala_pipeline().readStream(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 255, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o731.readStream.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'live.dim_products_view'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:11)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:524)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readStreamInput(Pipeline.scala:492)\n\tat com.databricks.pipelines.Pipeline.readStream(Pipeline.scala:740)\n\tat com.databricks.pipelines.Pipeline.readStream(Pipeline.scala:748)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy103.apply(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1939)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:317)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:315)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1815)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1937)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:168)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:320)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1937)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1936)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:412)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:328)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:326)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:310)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:405)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:153)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:139)\n\tat com.databricks.pipelines.graph.ChangeFlow.flowFuncResult$lzycompute(Flow.scala:988)\n\tat com.databricks.pipelines.graph.ChangeFlow.flowFuncResult(Flow.scala:988)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:229)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:228)\n\tat com.databricks.pipelines.graph.ChangeFlow.super$failure(Flow.scala:1386)\n\tat com.databricks.pipelines.graph.ChangeFlow.$anonfun$failure$1(Flow.scala:1386)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.graph.ChangeFlow.failure(Flow.scala:1386)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:252)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:252)\n\tat com.databricks.pipelines.graph.ChangeFlow.resolved(Flow.scala:988)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:401)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:398)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:453)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:449)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:620)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:601)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:259)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:113)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:240)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:318)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:314)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:755)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 739, in apply\n    return self.df_func()._jdf\n           ^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 1011, in <lambda>\n    _ScalaDataFrameFunc(lambda: read_stream(source)),\n                                ^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 627, in read_stream\n    pipeline.instance.get_scala_pipeline().readStream(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 255, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o731.readStream.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'live.dim_products_view'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:11)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:524)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readStreamInput(Pipeline.scala:492)\n\tat com.databricks.pipelines.Pipeline.readStream(Pipeline.scala:740)\n\tat com.databricks.pipelines.Pipeline.readStream(Pipeline.scala:748)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy103.apply(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1939)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:317)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:315)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1815)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1937)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:168)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:320)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1937)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1936)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:412)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:328)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:326)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:310)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:405)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:153)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:139)\n\tat com.databricks.pipelines.graph.ChangeFlow.flowFuncResult$lzycompute(Flow.scala:988)\n\tat com.databricks.pipelines.graph.ChangeFlow.flowFuncResult(Flow.scala:988)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:229)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:228)\n\tat com.databricks.pipelines.graph.ChangeFlow.super$failure(Flow.scala:1386)\n\tat com.databricks.pipelines.graph.ChangeFlow.$anonfun$failure$1(Flow.scala:1386)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.graph.ChangeFlow.failure(Flow.scala:1386)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:252)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:252)\n\tat com.databricks.pipelines.graph.ChangeFlow.resolved(Flow.scala:988)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:401)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:398)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:453)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:449)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:620)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:601)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:259)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:113)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:240)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:318)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:314)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:755)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dlt.apply_changes(\n",
    "  target = \"dim_products\",\n",
    "  source = \"LIVE.dim_products_view\",\n",
    "  keys = [\"product_id\"],\n",
    "  sequence_by = \"product_id\",\n",
    "  stored_as_scd_type = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf18444-353a-4ddf-bdf0-5857e6ddebd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}